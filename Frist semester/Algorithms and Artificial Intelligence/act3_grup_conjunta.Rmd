---
title: "Actividad 3 Grupal: Análisis Genómico"
author: "Grupo 8"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
---

# 1. Preparación de Datos

Los datos proporcionados han sido separados en tres archivos diferentes:

-   `gene_expression.csv`: los datos de expresión génica, sin indicar las etiquetas de las variables ni las clases.
-   `classes.csv`: indica las clases correspondientes a cada fila del archivo anterior.
-   `column_names.txt`: contiene los nombres de las columnas, es decir, los genes analizados.

Para realizar el análisis, los datos deben unificarse en un único conjunto. Por ello, lo primero es integrar y depurar la información.

Se utiliza la __imputación por mediana__ para tratar los valores ausentes (NAs). También realizamos una __transformación logarítmica__ para mitigar el efecto de los valores extremos.

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Carga de librerías
library(tidyverse)
library(caret)
library(randomForest)
library(uwot)
library(factoextra)
library(pheatmap)
library(rpart)
library(rpart.plot)
library(e1071)
library(ggplot2)
library(pROC)
library(RColorBrewer) 

# CARGA DE DATOS
genes_name <- read.table("column_names.txt", header = FALSE, stringsAsFactors = FALSE)$V1
gene_expr <- read.csv("gene_expression.csv", sep = ";", header = FALSE)
sample_info <- read.csv("classes.csv", sep = ";", header = FALSE)
colnames(sample_info) <- c("SampleID", "Class")

# LIMPIEZA DE NOMBRES 
genes_name_clean <- make.names(genes_name, unique = TRUE)
colnames(gene_expr) <- genes_name_clean
rownames(gene_expr) <- sample_info$SampleID

# IMPUTACIÓN DE VALORES AUSENTES (NAs)
if(sum(is.na(gene_expr)) > 0) {
  preProc <- preProcess(gene_expr, method = "medianImpute")
  gene_expr <- predict(preProc, gene_expr)
}

# TRANSFORMACIÓN LOGARÍTMICA
# Sumamos 1 para evitar log(0)
gene_expr <- log2(gene_expr + 1)

# CREACIÓN DEL DATASET FINAL
full_data <- gene_expr
full_data$Class <- as.factor(make.names(sample_info$Class))

cat("Dimensiones finales del dataset:", dim(full_data), "\n")
cat("Transformación Log2 aplicada correctamente.\n")
```

# 2. Métodos no supervisados

## 2.1. Reducción de la dimensionalidad

### PCA (Análisis de Componentes Principales)

Es una técnica estadística de aprendizaje automático no supervisado utilizada para reducir la dimensionalidad de conjuntos de datos complejos. Transforma variables correlacionadas en un conjunto más pequeño de nuevas variables no correlacionadas (componentes principales), conservando la mayor parte de la información o varianza original.

```{r pcs, message=FALSE, warning=FALSE}
# Filtramos genes constantes (varianza 0) para evitar errores en el escalado
gene_variances <- apply(gene_expr, 2, var)
const_genes <- gene_variances == 0
gene_expr_pca <- gene_expr[, !const_genes]

cat("Genes eliminados por varianza cero:", sum(const_genes), "\n")

# EJECUCIÓN DEL PCA
pca_res <- prcomp(gene_expr_pca, scale. = TRUE)

# PREPARACIÓN PARA GGPLOT
# A) Creamos un dataframe con los resultados
pca_df <- as.data.frame(pca_res$x)

# B) Añadimos la columna de grupos
pca_df$grupo <- full_data$Class 

# C) Calculamos la varianza explicada para los ejes
varianza_explicada <- round(summary(pca_res)$importance[2, 1:2] * 100, 2)
pc1_lab <- paste0("PC1 (", varianza_explicada[1], "%)")
pc2_lab <- paste0("PC2 (", varianza_explicada[2], "%)")

# VISUALIZACIÓN
ggplot(pca_df, aes(x = PC1, y = PC2, color = grupo)) +
  geom_point(size = 3, alpha = 0.8) + 
  labs(title = "Análisis de Componentes Principales (PCA)",
       subtitle = "Agrupación por Subtipo Molecular",
       x = pc1_lab, 
       y = pc2_lab,
       color = "Grupo") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5), 
    legend.position = "right",
    panel.grid.minor = element_blank()
  ) +
  scale_color_brewer(palette = "Set1")

```

__Interpretación del PCA__: En el Análisis de Componentes Principales generado se observa que el PC1 (r varianza_explicada[1]%) y el PC2 (r varianza_explicada[2]%) explican conjuntamente el r sum(varianza_explicada)% de la varianza total. Considerando el ruido inherente a los datos genéticos, este resultado es aceptable.


Se aprecia que el grupo AGH (rojo) se diferencia claramente, indicando un perfil de expresión génica distintivo. Los grupos CFB, CGC, CHC y HPB presentan cierto solapamiento, sugiriendo similitudes transcriptómicas. Cabe destacar la dispersión del grupo CFB (azul), que indica heterogeneidad interna, frente a la homogeneidad del grupo HPB (naranja). obtenidos del análisis de expresión génica muestran valores bastante más homogéneos.

### U-MAP

El UMAP (Uniform Manifold Approximation and Projection) es un algoritmo de aprendizaje automático utilizado para la reducción de dimensionalidad. Su función principal es comprimir datos complejos y masivos en un espacio de dos o tres dimensiones, facilitando su visualización. Sirve para identificar patrones, agrupamientos (clusters) y estructuras ocultas, conservando mejor la topología global de los datos que otros métodos.

```{r umap, warning=FALSE, message=FALSE}
set.seed(1234)
# Ejecución de UMAP
umap_fit <- umap(gene_expr, n_neighbors = 15, n_components = 2, min_dist = 0.1, verbose = FALSE)
umap_df <- data.frame(umap_fit)
colnames(umap_df) <- c("UMAP1", "UMAP2")
umap_df$Class <- full_data$Class

ggplot(umap_df, aes(x = UMAP1, y = UMAP2, color = Class)) +
  geom_point(size = 2, alpha = 0.7) +
  theme_minimal() +
  labs(title = "UMAP: Proyección no lineal", color = "Clase")

```

__Interpretación del UMAP__:Esta visualización de UMAP muestra una proyección bidimensional donde datos de alta dimensionalidad se han agrupado en cinco clústeres claramente definidos, correspondientes a las clases AGH, CFB, CGC, CHC y HPB. La separación entre grupos es nítida, lo que indica que las categorías poseen perfiles diferenciados y una alta cohesión interna.

La disposición espacial revela que la clase AGH (rojo) se encuentra aislada en el extremo derecho, mientras que CHC (azul) y HPB (rosa) dominan la parte superior. En contraste, CFB (oliva) y CGC (verde) se sitúan en la zona inferior izquierda, mostrando una mayor proximidad relativa entre ellas. Esta estructura sugiere que el algoritmo ha capturado con éxito las relaciones topológicas, facilitando la identificación de patrones y la clasificación precisa de las muestras. Cabe mencionar que hay dos puntos pertenecientes a los grupos HPB y CFB que aparecen agrupados junto con los del grupo CGC.

## 2.2. Clusterización

### Clustering jerárquico con pca

```{r jerarquico, message=FALSE, warning=FALSE}
# Calculamos la varianza acumulada
pr_var <- pca_res$sdev^2
prop_varex <- pr_var / sum(pr_var)
pca_var_cum <- cumsum(prop_varex)

# Colores por clase
n_clases <- length(levels(full_data$Class))
paleta_colores <- RColorBrewer::brewer.pal(n = max(3, n_clases), name = "Set1")
class_colors <- paleta_colores[as.numeric(full_data$Class)]

# Selección de PCs
num_pcs <- which(pca_var_cum >= 0.80)[1]
cat("Número de componentes seleccionados (80% varianza):", num_pcs, "\n")
pca_scores <- pca_res$x[, 1:num_pcs]

# Clustering
dist_matrix <- dist(pca_scores, method = "euclidean")
hc_pca <- hclust(dist_matrix, method = "ward.D2")

# Dendrograma
dend_order <- hc_pca$order

plot(hc_pca,
     labels = FALSE,
     main = "Clustering Jerárquico sobre PCA",
     xlab = "Muestras",
     sub = paste("Basado en los primeros", num_pcs, "PCs"),
     hang = -1)

for (i in seq_along(dend_order)) {
  idx_muestra <- dend_order[i]
  axis(side = 1, at = i, labels = FALSE, 
       col = class_colors[idx_muestra], 
       lwd = 2, las = 2, tck = -0.02)
}

legend("topright", legend = levels(full_data$Class), 
       fill = paleta_colores, bty = "n", cex = 0.8)

# Corte en k=5
k <- length(levels(full_data$Class))
rect.hclust(hc_pca, k = k, border = "red")

```

A partir de las componentes principales (PCs) que explican aproximadamente el 80 % de la varianza acumulada, se llevó a cabo un clustering jerárquico utilizando la distancia euclídea y el método de enlace Ward.D2. Este enfoque permite agrupar las muestras en función de las principales fuentes de variabilidad identificadas mediante el PCA, reduciendo la influencia del ruido presente en los datos originales.

El dendrograma obtenido muestra una estructura de agrupamiento razonablemente clara, con una homogeneidad interna globalmente aceptable. En particular, los clústeres 4 y 5 separan de forma completa (100 %) las muestras correspondientes a las clases AGH y HPB, respectivamente, lo que indica que estas condiciones presentan perfiles de expresión génica claramente diferenciados.

Por el contrario, los clústeres 1 a 3, aunque están dominados por una clase principal, contienen también muestras pertenecientes a otras clases. Concretamente, el clúster 1 está compuesto mayoritariamente por muestras de la clase CHC (86,6%), pero incluye un 12,7% de muestras de la clase CFB y un 0,64% de muestras de CGC. El clúster 2 está dominado por muestras de la clase CGC (75.7%), aunque presenta una proporción relevante de muestras de la clase CFB (23,8%) y una pequeña representación de HPB (0,5%). Por último, el clúster 3 está formado casi en su totalidad por muestras de la clase CFB (99,2%), con una presencia residual de muestras de AGH y CGC (0,4% en ambos casos).

Estos resultados indican que, aunque el uso de PCA previo al clustering mejora la coherencia de los agrupamientos, las diferencias transcriptómicas entre determinadas condiciones no son completamente discriminantes. Este comportamiento refleja la complejidad biológica del sistema analizado y pone de manifiesto las limitaciones inherentes a los métodos no supervisados para separar de forma perfecta clases biológicamente relacionadas.

### K-Means

El método K-Means consiste en la partición de una conjunto de $n$ muestras (en nuestro caso pacientes) en $K$ grupos o _clusters_, de manera que cada muestra pertenzca al grupo cuyo valor medio (centroide) esté más cercano. Fijamos $K=5$ según las clases teóricas conocidas.

```{r kmeans, message=FALSE, warning=FALSE}
set.seed(1234)

# Filtrado de genes constantes y escalado
gene_variances <- apply(gene_expr, 2, var)
gene_expr_clean <- gene_expr[, gene_variances > 0]
gene_expr_scaled <- scale(gene_expr_clean)

# Ejecución K-Means
km_res <- kmeans(gene_expr_scaled, centers = 5, nstart = 25)

# Visualización (Proyección PCA)
pca_k_vis <- prcomp(gene_expr_scaled, center = FALSE, scale. = FALSE)
df_kmeans <- data.frame(pca_k_vis$x[, 1:2])
colnames(df_kmeans) <- c("PC1", "PC2")
df_kmeans$Cluster <- as.factor(km_res$cluster)

ggplot(df_kmeans, aes(x = PC1, y = PC2, color = Cluster, fill = Cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  stat_ellipse(geom = "polygon", alpha = 0.1, type = "norm", level = 0.95) +
  scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "K-Means Clustering (K=5)",
       subtitle = "Proyección sobre Componentes Principales",
       x = "Dimensión 1 (PC1)",
       y = "Dimensión 2 (PC2)",
       color = "Cluster", fill = "Cluster") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))
```

Al igual que en el PCA, K-Means logra aislar bien ciertos grupos (correspondientes probablemente a AGH), pero muestra dificultades para separar nítidamente los grupos centrales superpuestos.

# 3. Métodos supervidados

Dividimos los datos en entrenamiento (70%) y prueba (30%) y configuramos la validación cruzada.

```{r partition, message=FALSE, warning=FALSE}
set.seed(1234)
trainIndex <- createDataPartition(full_data$Class, p = 0.7, list = FALSE)
trainData <- full_data[trainIndex, ]
testData  <- full_data[-trainIndex, ]

# Control de entrenamiento
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = "final")
```

## Random Forest

Rondom forest (RF) es el método basado en bagging (Agregación Bootstrap) más utilizado. La idea principal del RF es la combinación de un gran número de arboles de decisión en un gran bosque mediante bagging. 

En primer lugar, lo que tenemos que hacer es una partición de los datos, los cuales unos utilizaremos para el entrenamiento y el otro para el testeo. Esto lo hemos realizado al principio del Rmarkdown junto a la limpieza de los datos. 

El siguiente paso es el entrenamiento de nuestro modelo y el cálculo del número de predictores aleatorio óptimo. El cual representamos en una gráfica. 

```{r rf, message=FALSE, warning=FALSE}
set.seed(1234)
rf_model <- train(Class ~ ., data = trainData, method = "rf",
                  trControl = ctrl, preProcess = c("center", "scale"),
                  tuneLength = 3)
print(rf_model)

```

## Support Vector Machines (SVM)

Este método busca un hiperplano que separe las clases en el espacio de características de la manera más amplia posible. Las SVM son especialmente útiles para problemas de clasificación y regresión, y pueden manejar datos no lineales mediante el uso de diferentes funciones de kernel.

```{r svm, message=FALSE, warning=FALSE}
set.seed(1234)
svm_model <- train(Class ~ ., data = trainData, method = "svmRadial",
                   trControl = ctrl, preProcess = c("center", "scale"),
                   tuneLength = 3)
print(svm_model)

```

## Árbol de decisión (CART)

Un árbol de decisiones es un modelo predictivo y gráfico que organiza acciones, eventos aleatorios y sus consecuencias para facilitar la toma de decisiones complejas. Utiliza una estructura jerárquica con nodos de decisión (cuadrados), de probabilidad (círculos) y terminales (resultados) para evaluar costos, beneficios y probabilidades, permitiendo elegir la mejor opción.

```{r cart, message=FALSE, warning=FALSE}
set.seed(1234)
tree_model <- train(Class ~ ., data = trainData, method = "rpart",
                    trControl = ctrl, tuneLength = 5)
rpart.plot(tree_model$finalModel, main = "Árbol de Decisión Final")

```

### Interpetación del arbol de decisión

El árbol de decisión generado clasifica las muestras de cinco subtipos moleculares (AGH, CFB, CGC, CHC, HPB) basándose en niveles críticos de expresión génica. La raíz del árbol utiliza el gen `CPSF2`; valores superiores o iguales a 6.4 identifican directamente al grupo CHC.

Otras ramas emplean genes como `ATP5PD, EIF6 y RAB32` para filtrar los subtipos restantes. Por ejemplo, el nodo de `ATP5PD` aísla eficazmente al grupo AGH. EL marcador `EIF6` por su parte hace una diferenciación de el grupo CFB y CGC, aunque no es una diferenciación tan clara como la de ATP5PD. Asimismo, el marcador `RAB32` separa de una manera clara el grupo HPB de el grupo CGC. Cada nodo terminal muestra la probabilidad de pertenencia y el porcentaje de la muestra total, permitiendo identificar biomarcadores clave para el diagnóstico clínico.

# 4. Evaluación y comparativa

En este apratado comparamos los rendimientos de lso tres modelos en el conjunto de pruebas.

```{r eval, message=FALSE, warning=FALSE}
# Predicciones
pred_rf <- predict(rf_model, testData)
pred_svm <- predict(svm_model, testData)
pred_tree <- predict(tree_model, testData)

# Función para reporte detallado
mostrar_metricas <- function(preds, real, nombre) {
  cat("\n=== EVALUACIÓN: ", nombre, " ===\n")
  cm <- confusionMatrix(preds, real, mode = "everything")
  print(cm$table)
  cat("\nAccuracy Global:", round(cm$overall['Accuracy'], 4), "\n")
  # Mostramos F1 por clase transpuesto para legibilidad
  print(t(cm$byClass[, c("Sensitivity", "Precision", "F1")]))
}

mostrar_metricas(pred_rf, testData$Class, "RANDOM FOREST")

# Tabla Resumen
comp_table <- data.frame(
  Modelo = c("Random Forest", "SVM", "Árbol Decisión"),
  Accuracy = c(
    confusionMatrix(pred_rf, testData$Class)$overall['Accuracy'],
    confusionMatrix(pred_svm, testData$Class)$overall['Accuracy'],
    confusionMatrix(pred_tree, testData$Class)$overall['Accuracy']
  )
)

print(comp_table)
```

__Interpretación de la Matriz de Confusión__: En la matriz de confusión generada observamos cómo de bien clasifica el modelo las muestras. Las filas hacen referencia a los grupos reales y las columnas a los predichos. Los resultados muestran que más del 90% de las muestras han sido correctamente clasificadas.

# 5. Preguntas de la actividad. 
1.	Procesamiento de los datos:

- ¿Qué método habéis escogido para llevar a cabo la imputación de los datos? 
  
  Se realizó en método de __imputación por mediana__. Se usa este método ya que la mediana es una medida de tendencia central que no es tan sensible a valores atípicos como la media.
  
- ¿Habéis llevado a cabo algún otro tipo de procesamiento?

  A parte de lo anteriomente dicho hemos realizado una __tranformación logarítmica ($\log_2(x+1)$)__. Los datos de expresión génica tienen la particularidad de que seguir una distribución asimétrica, donde unos pocos genes tienen valores de expresión muy altos y la mayoría muy bajos. Esta transformación estabiliza esta varianza y reduce el sesgo. 


2.	Métodos no supervisados:

- ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de reducción de dimensionalidad? 

  Seleccionamos el PCA lineal porque es el método más común y nos sirve para entender la varianza global y detectar _outliers_. Además no es computacionalmente exigente. Por otro lado, elegimos el UMAP ya que es el método que mejores resultados nos ha dado debido a su capacidad para interpretar las estructuras biológicas complejas. Es computacionalmente más exigente que el PCA. 

- ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de clusterización?

  Seleccionamos el primer método de Clustering Jerárquico ya que es un estandar en trasncriptómica que nos permite visualizar las relaciones entre genes. El método k-means fué seleccionado por por su eficiencia computacional y capacidad para generar particiones "duras" (hard clustering). Al conocer a priori que existen 5 clases teóricas, K-Means es ideal para validar si la estructura matemática de los datos coincide con dichas clases forzando $k=5$."

- En ambos casos, ¿qué aspectos positivos y negativos tienen cada una? 
  
  - PCA: 
    Positivo: Rápido, interpretable (sabemos qué genes pesan en cada PC). 
    Negativo: Pierde información si las relaciones son no lineales, como es en nuestro caso.
  
  - UMAP: 
    Positivo: Excelente visualización de clusters separados, debido a su comprensión de las relaciones no lineales. 
    Negativo: Las distancias y densidades no se conservan perfectamente (no se puede interpretar la distancia en el eje X igual que en PCA) y puede consumir muchos recursos computacionales.
  
  - Jerárquico: 
    Positivo: No requiere definir $k$ de antemano, determinista. 
    Negativo: Computacionalmente costoso, y muy sensible al ruido.
  
  - K-Means: 
    Positivo: Muy rápido y sencillo. 
    Negativo: Requiere fijar $k$ previamente, y asume clusters esféricos.

- En el caso de la clusterización, ¿podéis afirmar con certeza que los clústeres generados son los mejores posibles?

  Sin dudad __no podemos afirmar que las opciones que hemos escogido sean las mejores__. Para ello habría que comparar todos lo métodos que conocemos cambiando los parámetros para ver cómo estos afectan a la interpretación. Y aún con todo esto puede que debiéramos usar más de un metodo a la vez para interpretar distintas caracteristicas de nuestra muestra. A pesar de esto consideramos que los resultados obtenidos son lo suficientemente buenos como para presentarlos ya que los clusters obtenidos coinciden en gran medida con las muetras reales. 

3.	Métodos supervisados:

- ¿Cuál es el motivo por el cual habéis seleccionado ambas técnicas de aprendizaje supervisado? ¿Cuál ha dado mejores resultados a la hora de clasificar las muestras?

  Seleccionamos Random Forest por ser un método de ensamblaje robusto que maneja bien el problema de que haya más genes que muestras y ofrece interpretabilidad. SVM se escogió por su capacidad para encontrar hiperplanos óptimos en espacios de alta dimensionalidad. Finalmente, el Árbol de Decisión se usó como línea base por su alta interpretabilidad clínica.Tras evaluar la matriz de confusión y las métricas en el conjunto de test, el algoritmo que ha dado mejores resultados ha sido __Random Forest__, obteniendo el mayor Accuracy y F1-Score promedio (0,99 y 0,99), lo que indica un equilibrio óptimo entre precisión y sensibilidad.

- ¿Habéis considerado oportuno implementar algún método de reducción de dimensionalidad para procesar los datos antes de implementarlos en dichas técnicas? ¿Por qué?

  En esta acitividad hemos optado por __itroducir los genes procesados directemente__, sin realizar una reduccción previa mediante PCA. Esto se debe a que algoritmos como el Random Fores realizan su propia selección de características internarnas, e ignora los genes irrelevantes. Por otro lado, creemos que __haber aplicado esta reducción previa en el caso de SVM hubiera podido  ayudarnos a reducir el ruido y el coste computacional__. 

- ¿Qué aspectos positivos y negativos tienen cada una de las técnicas que habéis escogido?

  - Random Forest: 
    - Positivo: Muy preciso, robusto al ruido, no requiere escalado estricto. 
    - Negativo: Difícil de interpretar completamente y lento de entrenar.

  - SVM: 
    - Positivo: Muy eficaz en alta dimensionalidad. 
    - Negativo: Muy sensible a la elección de hiperparámetros y al escalado de datos.

  - Árbol de Decisión: 
    - Positivo: Muy fácil de interpretar visualmente. 
    - Negativo: Tiende al sobreajuste y es inestable (pequeños cambios en los datos cambian el árbol).


4.	De estas cuatro opciones, ¿qué tipo de arquitectura de deep learning sería la más adecuada para procesar datos de expresión génica?
- __a) Red de perceptrones (multiperceptron layers)__. Los datos de expresión génica son datos tabulares/estructurados donde las características no tienen necesariamente una relación espacial (como los píxeles en una imagen, donde usaríamos CNNs) ni temporal (como en el audio, donde usaríamos RNNs). Por tanto, una red densa o MLP es la arquitectura estándar adecuada para aprender patrones no lineales complejos en este tipo de datos independientes.


